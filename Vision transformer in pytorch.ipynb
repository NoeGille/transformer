{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchEmbed(nn.Module):\n",
    "    \"\"\"Split image into patches and then embed them\"\"\"\n",
    "\n",
    "    def __init__(self, img_size:int=224, patch_size:int=16, in_channels:int=1, embed_dim:int=768):\n",
    "        super().__init__()\n",
    "        self.img_size = img_size\n",
    "        self.patch_size = patch_size\n",
    "        self.n_patches = (img_size // patch_size) ** 2\n",
    "\n",
    "        # Define a conv layer to extract patches from the image\n",
    "        self.proj = nn.Conv2d(in_channels, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        '''Transform the image into a tensor of patches'''\n",
    "        # (n_samples, in_channels, img_size, img_size)\n",
    "        x = self.proj(x) # (n_samples, embed_dim, n_patches**0.5, n_patches**0.5)\n",
    "        x = x.flatten(2) # (n_samples, embed_dim, n_patches)\n",
    "        x = x.transpose(1, 2)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    '''Attention mecanism'''\n",
    "\n",
    "    def __init__(self, dim, n_heads, qkv_bias:bool=False, attn_p:float=0., proj_p:float=0):\n",
    "        '''qkv_bias : If Ture then we include bias to the query, key adn value projections.\n",
    "\n",
    "        attn_p : Dropout probability applied to the query, key and value tensors.\n",
    "        proj_p : Dropout probability applied to the output tensor\n",
    "        Note : Dropout is only applied during training, not during evaluation or prediction'''\n",
    "        super().__init__()\n",
    "        self.n_heads = n_heads\n",
    "        self.dim = dim\n",
    "        self.head_dim = dim // n_heads\n",
    "        self.scale = self.head_dim ** -0.5\n",
    "        # Input : embedding | Output : query, key and value vectors of the embedding\n",
    "        # Note : We could write three seperate linear mapping that do the same thing\n",
    "        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n",
    "        self.attn_drop = nn.Dropout(attn_p)\n",
    "        # Take the concatenates heads and map them into a new space\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "        self.proj_drop = nn.Dropout(proj_p)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        n_samples, n_tokens, dim = x.shape\n",
    "        if dim != self.dim:\n",
    "            raise ValueError\n",
    "        \n",
    "        qkv = self.qkv(x) # (n_samples, n_patches + 1, 3 * dim)\n",
    "        qkv = qkv.reshape(\n",
    "            n_samples, n_tokens, 3, self.n_heads, self.head_dim\n",
    "        ) # (n_samples, n_patches + 1, 3, n_heads, head_dim)\n",
    "        qkv = qkv.permute(2, 0, 3, 1, 4) # (3, n_samples, n_heads, n_patches + 1, head_dim)\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
    "        k_t = k.transpose(-2, -1)\n",
    "        dot_product = (\n",
    "            q @ k_t\n",
    "        ) * self.scale # (n_samples, n_heads, n_patches + 1, n_patches + 1)\n",
    "        attn = dot_product.softmax(dim=-1) # (n_samples, n_heads, n_patches + 1, n_patches + 1)\n",
    "        attn = self.attn_drop(attn)\n",
    "        weighted_avg = attn @ v # (n_samples, n_heads, n_patches + 1, head_dim)\n",
    "        # Flatten last 2 dimension <=> Concatenate each head output\n",
    "        weighted_avg = weighted_avg.transpose(1, 2) # (n_samples, n_patches + 1, n_heads, head_dim)\n",
    "        # head_dim = dim // n_heads => Get the same dimesion as input\n",
    "        weighted_avg = weighted_avg.flatten(2) # (n_samples, n_patches + 1, dim)\n",
    "        # Final linear projection and dropout\n",
    "        x = self.proj(weighted_avg) # (n_samples, n_patches + 1, dim)\n",
    "        x = self.proj_drop(x) # (n_samples, n_patches + 1, dim)\n",
    "\n",
    "        return x\n",
    "    \n",
    "class MLP(nn.Module):\n",
    "    '''MultiLayer Perception'''\n",
    "    def __init__(self, in_features, hidden_features, out_features, p=0.):\n",
    "        '''One hidden layer'''\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(in_features, hidden_features)\n",
    "        self.activation = nn.GELU()\n",
    "        self.fc2 = nn.Linear(hidden_features, out_features)\n",
    "        self.dropout = nn.Dropout(p)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.dropout(self.fc2(self.dropout(self.activation(self.fc1(x)))))\n",
    "\n",
    "\n",
    "class Block(nn.Module):\n",
    "\n",
    "    def __init__(self, dim, n_heads, mlp_ratio, qkv_bias, p, attn_p):\n",
    "        '''mlp_ratio : determine the hidden dimension size of the mlp module with respect to dim'''\n",
    "        super().__init__()\n",
    "        self.norm1 = nn.LayerNorm(dim, eps=1e-6)\n",
    "        self.norm2 = nn.LayerNorm(dim, eps=1e-6)\n",
    "        self.attn = Attention(dim, n_heads, qkv_bias, attn_p, proj_p=p)\n",
    "        self.mlp = MLP(\n",
    "            in_features=dim, \n",
    "            hidden_features=int(dim * mlp_ratio), \n",
    "            out_features=dim\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.norm1(x))\n",
    "        x = x + self.mlp(self.norm2(x))\n",
    "        return x\n",
    "\n",
    "class VisionTransformer(nn.Module):\n",
    "    '''Vision transformer'''\n",
    "    def __init__(self, img_size=384, patch_size=16, in_chans=3, n_classes=1000, \n",
    "                 embed_dim=768, depth=12, n_heads=4, mlp_ratio=4., qkv_bias=True, p=0., attn_p=0.):\n",
    "        super().__init__()\n",
    "        self.patch_embed = PatchEmbed(img_size=img_size, patch_size=patch_size, in_channels=in_chans,embed_dim=embed_dim)\n",
    "        # Learnable parameter taht will represent the first token in the sequence. It has embed_dim elements.\n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
    "        self.pos_embed = nn.Parameter(torch.zeros(1, 1 + self.patch_embed.n_patches, embed_dim))\n",
    "        self.pos_drop = nn.Dropout(p=p)\n",
    "        self.blocks = nn.ModuleList(\n",
    "            [\n",
    "                Block(\n",
    "                    dim=embed_dim,\n",
    "                    n_heads=n_heads,\n",
    "                    mlp_ratio=mlp_ratio,\n",
    "                    qkv_bias=qkv_bias,\n",
    "                    p=p,\n",
    "                    attn_p=attn_p\n",
    "                )\n",
    "                for _ in range(depth)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self.norm = nn.LayerNorm(embed_dim, eps=1e-6)\n",
    "        self.head = nn.Linear(embed_dim, n_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Transform input images into patch embedding\n",
    "        n_samples = x.shape[0]\n",
    "        x = self.patch_embed(x)\n",
    "        # Replicates the class token over the sample dimension\n",
    "        cls_token = self.cls_token.expand(n_samples, -1, -1) # (n_samples, 1, embed_dim)\n",
    "        x = torch.cat((cls_token, x), dim=1) # (n_samples, 1 + n_patches, embed_dim)\n",
    "        x = x + self.pos_embed # (n_samples, 1 + n_patches, embed_dim)\n",
    "        x = self.pos_drop(x)\n",
    "\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "\n",
    "        x = self.norm(x)\n",
    "        cls_token_final = x[:, 0] # just the cls token\n",
    "        x = self.head(cls_token_final)\n",
    "        return x"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import MNIST Dataset to test the model on classification task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.0\n"
     ]
    }
   ],
   "source": [
    "from torchvision.datasets import MNIST\n",
    "import torchvision.transforms as transforms\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "# import matplotlib\n",
    "# matplotlib.use('TkAgg') # Necessary to run matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "\n",
    "import albumentations as A\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from torch.nn import functional as F\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim\n",
    "\n",
    "print(torch.__version__)\n",
    "import copy\n",
    "\n",
    "from collections import OrderedDict\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "\n",
    "train_dataset = MNIST(root='',train = True, download = True, transform=transforms.ToTensor())\n",
    "valid_dataset = MNIST(root='', train = False, download = True, transform=transforms.ToTensor())\n",
    "\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_dataloader = DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3750/3750 [01:12<00:00, 52.08it/s]\n",
      "100%|██████████| 3750/3750 [01:11<00:00, 52.28it/s]\n",
      "100%|██████████| 3750/3750 [01:11<00:00, 52.29it/s]\n"
     ]
    }
   ],
   "source": [
    "# Image width equals image height\n",
    "IMG_SIZE = 28\n",
    "NUM_CLASSES = 10\n",
    "EPOCHS = 10\n",
    "LEARNING_RATE = 0.01\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model = VisionTransformer(\n",
    "    img_size=28,\n",
    "    patch_size=7,\n",
    "    in_chans=1,\n",
    "    n_classes=NUM_CLASSES,\n",
    "    embed_dim=8,\n",
    "    depth=10,\n",
    "    n_heads=4\n",
    ").to(device=DEVICE)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    #metrics_calculator = SegmentationMetrics()\n",
    "    i = 0\n",
    "    for img, y in tqdm(train_dataloader):\n",
    "        \n",
    "        img = img.to(device=DEVICE)\n",
    "        y = y.to(device=DEVICE)\n",
    "        # prediction\n",
    "        y_pred = model(img)  # dim : (batch_size, 10, 224, 224)\n",
    "        # Calculate loss\n",
    "        loss = criterion(y_pred, y)\n",
    "\n",
    "        # backward\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        # gradient descent or adam step\n",
    "        optimizer.step()\n",
    "        i += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 625/625 [00:04<00:00, 128.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9159\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    predictions = []\n",
    "    grountruths = []\n",
    "    for X, y in tqdm(test_dataloader):\n",
    "        X = X.to(device=DEVICE)\n",
    "        pred = model(X)\n",
    "        predictions = predictions + pred.argmax(axis=1).cpu().numpy().tolist()\n",
    "        grountruths = grountruths + list(y)\n",
    "    print((np.array(predictions) == np.array(grountruths)).sum() / len(predictions))\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpu",
   "language": "python",
   "name": "gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
